{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMR0WSnZqf2MZPgtXrTpLez"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LlKD0clrEPim"},"outputs":[],"source":["!pip install pycocotools --quiet\n","!git clone https://github.com/pytorch/vision.git\n","!git checkout v0.3.0\n","\n","!cp vision/references/detection/utils.py ./\n","!cp vision/references/detection/transforms.py ./\n","!cp vision/references/detection/coco_eval.py ./\n","!cp vision/references/detection/engine.py ./\n","!cp vision/references/detection/coco_utils.py ./"]},{"cell_type":"code","source":["import zipfile\n","import torch\n","import torch.nn as nn\n","import os\n","import cv2 as cv\n","import torch.optim as optim\n","from torchvision import transforms\n","import numpy as np\n","import torch.nn.functional as F\n","import timeit\n","import pickle\n","import matplotlib.pyplot as plt\n","import torchvision.models as models\n","from google.colab.patches import cv2_imshow\n","\n","import random\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')\n","from xml.etree import ElementTree as et\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torch\n","import torchvision\n","from torchvision import transforms as torchtrans\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import sys\n","\n","# these are the helper libraries imported.\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","# for image augmentations\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2"],"metadata":{"id":"lw68rpyiEb39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extragerea datelor de antrenare pentru task-urile 1 si 2"],"metadata":{"id":"k-MKtTY9Efcy"}},{"cell_type":"code","source":["def extragere_exemple_pozitive():\n","    nr_img = 0\n","    folders = [\"barney\", \"betty\", \"fred\", \"wilma\"]\n","\n","    for folder_name in folders:\n","        cale_txt = os.path.join(\"antrenare\", f\"{folder_name}_annotations.txt\")\n","        continut_txt = np.loadtxt(cale_txt, dtype='str')\n","\n","        for i, linie in enumerate(continut_txt):\n","            nr_img += 1\n","            cale_img = os.path.join(\"antrenare\", f\"{folder_name}\", linie[0])\n","            short_path = f\"img_{nr_img:05}.jpg\"\n","            cale_output = os.path.join(\"data\", \"exemplePozitive\", short_path)\n","\n","            imagine = cv.imread(cale_img)\n","\n","            coord = np.array(linie[1:5], np.int32)\n","            chip_frumos = cv.resize(imagine[coord[1]:coord[3], coord[0]:coord[2]].copy(), (36,36))\n","            cv.imwrite(cale_output, chip_frumos)\n"],"metadata":{"id":"Dp7NJmFgEjvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extragere_exemple_negative(num_regions=3, region_size=(36, 36)):\n","    folders = [\"barney\", \"betty\", \"fred\", \"wilma\"]\n","\n","    for folder_name in folders:\n","        cale_txt = os.path.join(\"antrenare\", f\"{folder_name}_annotations.txt\")\n","        continut_txt = np.loadtxt(cale_txt, dtype='str')\n","\n","        for i in range(1000):\n","            short_path = f\"{i+1:04}.jpg\"\n","            cale_img = os.path.join(\"antrenare\", f\"{folder_name}\", short_path)\n","            imagine = cv.imread(cale_img)\n","            height, width, _ = imagine.shape\n","\n","            indici_per_poza = np.where(continut_txt[:, 0] == short_path)[0]\n","            face_boxes = [tuple(np.array(continut_txt[idx][1:5], np.int32)) for idx in indici_per_poza]\n","\n","            non_face_regions = []\n","\n","            while len(non_face_regions) < num_regions:\n","                rand_top = np.random.randint(0, height - region_size[0])\n","                rand_left = np.random.randint(0, width - region_size[1])\n","\n","                overlaps_with_face = any(\n","                    not (rand_left + region_size[1] < xmin or rand_top + region_size[0] < ymin or\n","                         rand_left > xmax or rand_top > ymax)\n","                    for xmin, ymin, xmax, ymax in face_boxes\n","                )\n","\n","                if not overlaps_with_face:\n","                    non_face_regions.append((rand_left, rand_top, region_size[1], region_size[0]))\n","\n","            for j, (left, top, width, height) in enumerate(non_face_regions):\n","                region = imagine[top:top + height, left:left + width]\n","                cale_output = os.path.join(\"data\", \"exempleNegative\", f\"{folder_name}_{i+1:04}_{j}.jpg\")\n","                cv.imwrite(cale_output, region)\n"],"metadata":{"id":"7z5KnzLnH9V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","data_path = \"\"\n","train_path_pozitive = data_path + \"/exemplePozitive\"\n","train_path_negative = data_path + \"/exempleNegative\"\n"],"metadata":{"id":"n4WaG2-ZI-v-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Antrenarea modelelor"],"metadata":{"id":"CmlhQS0_HyRZ"}},{"cell_type":"code","source":["model_alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n","model_barney = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n","model_betty = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n","model_fred = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n","model_wilma = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)\n","model_unknown = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)"],"metadata":{"id":"CRI7iFV5H4Lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nr_filters = model_alexnet.classifier[4].out_features\n","model_alexnet.classifier[6] = nn.Linear(nr_filters, 2)\n","model_barney.classifier[6] = nn.Linear(nr_filters, 2)\n","model_betty.classifier[6] = nn.Linear(nr_filters, 2)\n","model_fred.classifier[6] = nn.Linear(nr_filters, 2)\n","model_wilma.classifier[6] = nn.Linear(nr_filters, 2)\n","model_unknown.classifier[6] = nn.Linear(nr_filters, 2)"],"metadata":{"id":"qrTuQjOoIRZx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 1"],"metadata":{"id":"9XvZNsNkJ58h"}},{"cell_type":"code","source":["list_images_paths_pozitive = os.listdir(train_path_pozitive)\n","list_images_paths_negative = os.listdir(train_path_negative)"],"metadata":{"id":"lrnSbSfTKP0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = []\n","current_label = 1 # positive images\n","\n","for image_name in list_images_paths_pozitive:\n","    image = cv.imread(train_path_pozitive + \"/\" + image_name)\n","    train_dataset.append((image, current_label))"],"metadata":{"id":"taTIEeldJ-30"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_label = 0 # negative images\n","\n","for image_name in list_images_paths_negative:\n","    image = cv.imread(train_path_pozitive + \"/\" + image_name)\n","    train_dataset.append((image, current_label))"],"metadata":{"id":"ZLTB-9cVKlb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def resize_image(image, size=64):\n","    resized_image = cv.resize(image, (size, size))\n","    return resized_image\n","\n","\n","transformed_data = []\n","for image, label in train_dataset:\n","\n","    transformed_image = resize_image(image)\n","    transformed_image = torch.tensor(transformed_image).permute(2, 0, 1).float()\n","    transformed_image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transformed_image)\n","    transformed_data.append((transformed_image, label))\n"],"metadata":{"id":"TrCQnqLoKvop"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","data_loader = {\n","    'train' : torch.utils.data.DataLoader(transformed_data[:15000], batch_size=100, shuffle=True, num_workers=1),\n","    'validation' : torch.utils.data.DataLoader(transformed_data[15000:], batch_size=100, shuffle=True, num_workers=1)\n","}"],"metadata":{"id":"_MvVgIiHK5p8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if(torch.cuda.is_available()):\n","    model_alexnet = model_alexnet.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model_alexnet.parameters(), lr=0.001, momentum=0.9)\n","\n","total = 0\n","correct = 0\n","start = time.time()\n","\n","for epoch in range(num_epochs):\n","\n","    for i, data in enumerate(data_loader['train']):\n","        images, labels = data\n","        model_alexnet.train()\n","        if(torch.cuda.is_available()):\n","            images = images.cuda()\n","            labels = labels.cuda()\n","\n","        optimizer.zero_grad()\n","        outputs = model_alexnet(images.float())\n","\n","        _, predicted = torch.max(outputs, 1)\n","\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        labels = labels.type(torch.LongTensor)\n","        labels = labels.to(device)\n","        loss = criterion(outputs, labels)\n","        if(i % 50 == 0):\n","            print(f'Epoca: {epoch} Batch: {i} loss: {loss.item()}')\n","            print(f'Acuratete antrenare: {(correct/total)*100}')\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    with torch.no_grad():\n","        model_alexnet.eval()\n","        validation_correct = 0\n","        validation_total = 0\n","        for i, data in enumerate(data_loader['validation'], 1):\n","            images, labels = data\n","            if(torch.cuda.is_available()):\n","                images = images.cuda()\n","                labels = labels.cuda()\n","\n","            outputs = model_alexnet(images.float())\n","            _, predicted = torch.max(outputs, 1)\n","\n","            validation_total += labels.size(0)\n","            validation_correct += (predicted == labels).sum().item()\n","            if(i % 10 == 0):\n","                print(f'Acuratete validare: {validation_correct/validation_total)*100}\\n\\n')\n","\n","\n","print(f'Timp antrenare {time.time()-start} secunde')\n","print(f'Acuratete antrenare {(correct/total)*100}')\n","\n"],"metadata":{"id":"g4rWvl9QLH0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model_alexnet.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/model_alexnet_task1.pth')"],"metadata":{"id":"hMdCAu1iM5C_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 2"],"metadata":{"id":"PMHUyq1FNEKL"}},{"cell_type":"code","source":["dict_characters = {\n","    \"barney\": [],\n","    \"betty\": [],\n","    \"fred\": [],\n","    \"wilma\": [],\n","    \"unknown\": []\n","}\n","\n","dict_characters_transformed = {\n","    \"barney\": [],\n","    \"betty\": [],\n","    \"fred\": [],\n","    \"wilma\": [],\n","    \"unknown\": []\n","}\n","\n","labels = {\n","    \"barney\": 0,\n","    \"betty\": 1,\n","    \"fred\": 2,\n","    \"wilma\": 3,\n","    \"unknown\": 4\n","}\n","\n","transformed_list_labels = []"],"metadata":{"id":"Iw2yhTe3IrLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for elem, lista_elem in dict_characters.items():\n","    elem_images_path = os.listdir(f\"/content/drive/MyDrive/Colab Notebooks/proiect_cava2/train_{elem}\")\n","    print(f\"{elem} are {len(elem_images_path)} imagini\")\n","\n","    for image_name in elem_images_path:\n","        image = cv.imread(f\"/content/drive/MyDrive/Colab Notebooks/proiect_cava2/train_{elem}/{image_name}\")\n","        lista_elem.append(image)\n","\n","        transformed_image = torch.tensor(image.copy()).permute(2, 0, 1).float()\n","        transformed_image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transformed_image)\n","\n","        transformed_list_labels.append((transformed_image, labels[elem]))"],"metadata":{"id":"yFNoUo62IXBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, face_model in enumerate([model_barney, model_betty, model_fred, model_wilma, model_unknown]):\n","    if(torch.cuda.is_available()):\n","        face_model = face_model.cuda()\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(face_model.parameters(), lr=0.001, momentum=0.9)\n","\n","    train_model_data = []\n","    for j in range(len(transformed_list_labels)):\n","        # daca e din clasa pe care vrem sa o antrenam ii trecem clasa 1, altfel 0\n","        if transformed_list_labels[j][1] == idx:\n","            ind_class = 1\n","        else:\n","            ind_class = 0\n","        train_model_data.append((transformed_list_labels[j][0], ind_class))\n","    print(np.shape(train_model_data))\n","\n","    data_loader_class = torch.utils.data.DataLoader(train_model_data,\n","                                          batch_size=100,\n","                                          shuffle=True,\n","                                          num_workers=1)\n","\n","    print(f'Antrenare {face_model}')\n","    num_epochs = 10\n","    total = 0\n","    correct = 0\n","    start = time.time()\n","\n","    for epoch in range(num_epochs):\n","\n","        for i, data in enumerate(data_loader_class):\n","            images, labels = data\n","            face_model.train()\n","            if(torch.cuda.is_available()):\n","                images = images.cuda()\n","                labels = labels.cuda()\n","\n","            optimizer.zero_grad()\n","            outputs = face_model(images.float())\n","            _, predicted = torch.max(outputs, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","\n","            labels = labels.type(torch.LongTensor)\n","            labels = labels.to(device)\n","            loss = criterion(outputs, labels)\n","            if(i % 50 == 0):\n","                print(f'Epoca: {epoch} Batch: {i} loss: {loss.item()}')\n","                print(f'Acuratete antrenare: {(correct/total)*100}')\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","    print(f'Timp antrenare {time.time()-start} secunde')\n","    print(f'Acuratete antrenare {(correct/total)*100}')\n","\n"],"metadata":{"id":"Z1kNdAFDNmvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model_barney.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/modeL_barney_alexnet.pth')\n","torch.save(model_betty.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/model_betty_alexnet.pth')\n","torch.save(model_fred.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/model_fred_alexnet.pth')\n","torch.save(model_wilma.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/model_wilma_alexnet.pth')\n","torch.save(model_unknown.state_dict(), '/content/drive/MyDrive/Colab Notebooks/proiect_cava2/model_unknown_alexnet.pth')\n"],"metadata":{"id":"5HRaa4LSOfTD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Antrenarea modelului Faster-RCNN"],"metadata":{"id":"lSVsXyOeOjL3"}},{"cell_type":"code","source":["class FlintstoneImagesDataset(torch.utils.data.Dataset):\n","    def __init__(self, files_dir, labels_file, transforms=None):\n","        self.transforms = transforms\n","        self.files_dir = files_dir\n","        self.labels_file = labels_file\n","\n","        # Citirea etichetelor din fișierul text\n","        self.annotations = self.read_annotations(labels_file)\n","\n","        # Clasele (presupunem că sunt distincte și ordonate)\n","        self.classes = ['_', 'barney', 'betty', 'fred', 'wilma', 'unknown']\n","\n","    def read_annotations(self, labels_file):\n","        annotations = {}\n","        with open(labels_file, 'r') as file:\n","            for line in file:\n","                parts = line.strip().split()\n","                img_name, bbox = parts[0], list(map(int, parts[1:5]))\n","                class_name = parts[5]\n","                if img_name not in annotations:\n","                    annotations[img_name] = []\n","                annotations[img_name].append(bbox + [class_name])\n","        return annotations\n","\n","    def __getitem__(self, idx):\n","        img_name = sorted(list(self.annotations.keys()))[idx]\n","        image_path = os.path.join(self.files_dir, img_name)\n","        img = cv2.imread(image_path)\n","        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img_res = img_rgb / 255.0  # Normalize the image\n","\n","        boxes = []\n","        labels = []\n","        for box in self.annotations[img_name]:\n","            xmin, ymin, xmax, ymax, class_name = box\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            labels.append(self.classes.index(class_name))\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n","        image_id = os.path.splitext(img_name)[0]\n","        target = {\"boxes\": boxes, \"labels\": labels, \"area\": area, \"iscrowd\": iscrowd, \"image_id\": image_id}\n","        # target = {\"boxes\": boxes, \"labels\": labels, \"area\": area, \"iscrowd\": iscrowd, \"image_id\": torch.tensor([idx])}\n","\n","        if self.transforms:\n","            sample = self.transforms(image=img_res, bboxes=boxes, labels=labels)\n","            img_res = sample['image']\n","            target['boxes'] = torch.Tensor(sample['bboxes'])\n","\n","        return img_res, target\n","\n","    def __len__(self):\n","        return len(self.annotations)"],"metadata":{"id":"nXB5uS6YOocD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_object_detection_model(num_classes):\n","\n","    # load a model pre-trained pre-trained on COCO\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model"],"metadata":{"id":"-IbDPWSQOxcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"],"metadata":{"id":"s2NW7NBpO4W9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = FlintstoneImagesDataset(files_dir, labels_file, transforms=get_transform())\n","\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","\n","test_split = 0.2\n","tsize = int(len(dataset)*test_split)\n","dataset = torch.utils.data.Subset(dataset, indices[:-tsize])\n","dataset_test = torch.utils.data.Subset(dataset, indices[-tsize:])\n","\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=10, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n","    collate_fn=utils.collate_fn)"],"metadata":{"id":"xl82YygxO9cL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","num_classes = 6\n","model = get_object_detection_model(num_classes)\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"],"metadata":{"id":"x0NxPvH2PGA4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    lr_scheduler.step()\n","    evaluate(model, data_loader_test, device=device)"],"metadata":{"id":"thaK8G9IPQDN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n","    'epoch': epoch,\n","}, 'model_epoch_{}.pth'.format(epoch))\n"],"metadata":{"id":"qRttvtXCPVQ-"},"execution_count":null,"outputs":[]}]}